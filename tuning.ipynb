{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c074b8e",
   "metadata": {},
   "source": [
    "# Hyper-parameters Tuning\n",
    "\n",
    "- This is a supplement material for the [Machine Learning Simplified](https://themlsbook.com) book. It sheds light on Python implementations of the topics discussed while all detailed explanations can be found in the book. \n",
    "- I also assume you know Python syntax and how it works. If you don't, I highly recommend you to take a break and get introduced to the language before going forward with my code. \n",
    "- This material can be downloaded as a Jupyter notebook (Download button in the upper-right corner -> `.ipynb`) to reproduce the code and play around with it. \n",
    "\n",
    "\n",
    "This notebook is a supplement for *Chapter 12. Model Tuning and Selection* of **Machine Learning For Everyone** book.\n",
    "\n",
    "## 1. Required Libraries\n",
    "\n",
    "This block imports all necessary libraries. numpy is used for array manipulations, sklearn provides tools for data mining and data analysis, and skopt is used for Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a46c0e",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e9555",
   "metadata": {},
   "source": [
    "Here, we generate a synthetic dataset with 1000 samples and 20 features, split into training and test sets. This dataset will be used to train and evaluate our Decision Tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff32308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary classification dataset.\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f2202",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning with GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369387a",
   "metadata": {},
   "source": [
    "GridSearchCV exhaustively searches through the defined parameter grid, evaluating model performance for each combination using cross-validation. The best parameters and their corresponding performance are then displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_grid=param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bfbcf",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb25c93",
   "metadata": {},
   "source": [
    "RandomizedSearchCV offers a probabilistic approach, randomly selecting combinations from the parameter distribution. It's typically faster than GridSearchCV, especially when dealing with a large hyperparameter space or when every incremental improvement is not critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db55de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': np.arange(2, 21),\n",
    "    'min_samples_leaf': np.arange(1, 7)\n",
    "}\n",
    "\n",
    "# Initialize the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_distributions=param_dist, n_iter=100, cv=5, verbose=1, random_state=42, scoring='accuracy')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db8b5",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a5ed0",
   "metadata": {},
   "source": [
    "BayesSearchCV utilizes Bayesian optimization to search for optimal parameters. This method builds a probability model of the objective function and uses it to select the most promising parameters to evaluate in the true objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f84655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space\n",
    "param_space = {\n",
    "    'max_depth': Integer(10, 50),\n",
    "    'min_samples_split': Integer(2, 20),\n",
    "    'min_samples_leaf': Integer(1, 6)\n",
    "}\n",
    "\n",
    "# Initialize the BayesSearchCV object\n",
    "bayes_search = BayesSearchCV(estimator=DecisionTreeClassifier(random_state=42), search_spaces=param_space, n_iter=32, cv=5, verbose=1, scoring='accuracy')\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", bayes_search.best_params_)\n",
    "print(\"Best score: \", bayes_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = bayes_search.predict(X_test)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
