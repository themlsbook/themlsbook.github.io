
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bias-variance Decomposition &#8212; The Machine Learning Simplified book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://c6.patreon.com/becomePatronButton.bundle.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Cross-validation Methods" href="validation_methods.html" />
    <link rel="prev" title="Regularization" href="../chapter4/regularization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/mls_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Machine Learning Simplified book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   About
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  CHAPTER 2 - Overview of Supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter2/knn.html">
   K-Nearest Neighbors (KNN) Classifier
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  CHAPTER 3 - Model Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/cost_function.html">
   Cost Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter3/gradient_descent.html">
   Gradient Descent
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  CHAPTER 4 - Basis Expansion and Regularization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter4/basis_expansion.html">
   Basis Expansion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter4/regularization.html">
   Regularization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  CHAPTER 5 - Model Selection
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Bias-variance Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="validation_methods.html">
   Cross-validation Methods
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  CHAPTER 6 - Feature Selection
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter6/filter_methods.html">
   Filter Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapter6/wrapper_methods.html">
   Search Methods
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/chapter5/bias_variance_decomposition.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapter5/bias_variance_decomposition.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/5x12/themlsbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/5x12/themlsbook/issues/new?title=Issue%20on%20page%20%2Fchapter5/bias_variance_decomposition.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/5x12/themlsbook/edit/master/jupyter_book/chapter5/bias_variance_decomposition.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/5x12/themlsbook/master?urlpath=tree/jupyter_book/chapter5/bias_variance_decomposition.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#required-libraries-functions">
   1. Required Libraries &amp; Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthetic-data">
   2. Synthetic Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-synthetic-data-and-target">
     2.1. Define Synthetic Data and Target
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-two-datasets">
     2.2. Plot two datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fit-polynomials-of-different-degrees-to-two-datasets">
     2.3. Fit polynomials of different degrees to two datasets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance-computation">
   3. Variance Computation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-comptuation">
   4. Bias Comptuation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#total-epe">
   5. Total EPE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-test-error">
   6. Empirical Test Error
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   7. Conclusion
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Bias-variance Decomposition</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#required-libraries-functions">
   1. Required Libraries &amp; Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthetic-data">
   2. Synthetic Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-synthetic-data-and-target">
     2.1. Define Synthetic Data and Target
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-two-datasets">
     2.2. Plot two datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fit-polynomials-of-different-degrees-to-two-datasets">
     2.3. Fit polynomials of different degrees to two datasets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance-computation">
   3. Variance Computation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-comptuation">
   4. Bias Comptuation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#total-epe">
   5. Total EPE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-test-error">
   6. Empirical Test Error
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   7. Conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="bias-variance-decomposition">
<span id="chapter5-part1"></span><h1>Bias-variance Decomposition<a class="headerlink" href="#bias-variance-decomposition" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>This is a supplement material for the <a class="reference external" href="https://themlsbook.com">Machine Learning Simplified</a> book. It sheds light on Python implementations of the topics discussed while all detailed explanations can be found in the book.</p></li>
<li><p>I also assume you know Python syntax and how it works. If you don’t, I highly recommend you to take a break and get introduced to the language before going forward with my code.</p></li>
<li><p>This material can be downloaded as a Jupyter notebook (Download button in the upper-right corner -&gt; <code class="docutils literal notranslate"><span class="pre">.ipynb</span></code>) to reproduce the code and play around with it.</p></li>
</ul>
<div class="section" id="required-libraries-functions">
<h2>1. Required Libraries &amp; Functions<a class="headerlink" href="#required-libraries-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39; # sharper plots
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="synthetic-data">
<h2>2. Synthetic Data<a class="headerlink" href="#synthetic-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="define-synthetic-data-and-target">
<h3>2.1. Define Synthetic Data and Target<a class="headerlink" href="#define-synthetic-data-and-target" title="Permalink to this headline">¶</a></h3>
<p>Our synthetic problem defines:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f\)</span> is the true function.  <span class="math notranslate nohighlight">\(f\)</span> is usually not known, but for our synthetic example it is known and set to:
$<span class="math notranslate nohighlight">\(
  f(x) = \sin(x)
  \)</span>$</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_\epsilon\)</span>: the variance of the normally distributed observation noise <span class="math notranslate nohighlight">\(N(0, \sigma_\epsilon)\)</span>.  The observed value is equal to the  so the observation is distributed as:
$<span class="math notranslate nohighlight">\(
  y(x) \sim f(x) + N(0, \sigma_\epsilon)
  \)</span>$</p></li>
<li><p><span class="math notranslate nohighlight">\(p(x)\)</span> is a probability distribution over the data points <span class="math notranslate nohighlight">\(x\)</span>.  Here it is a uniform distribution on the interval <span class="math notranslate nohighlight">\([0, 2 \pi]\)</span>
$<span class="math notranslate nohighlight">\(
  p(x) \sim 
  \begin{cases}
      \frac{1}{2 \pi} &amp; \text{if } x \in [0, 2 \pi] \\ % &amp; is your &quot;\tab&quot;-like command (it's a tab alignment character)
      0 &amp; \text{otherwise.}
  \end{cases}
  \)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span>   <span class="c1"># range of x variable</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># true function</span>
<span class="n">sigma_eps</span> <span class="o">=</span> <span class="mf">0.25</span>            <span class="c1"># random noise (irreducible)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>             <span class="c1"># training set size</span>

<span class="k">def</span> <span class="nf">sample_x</span><span class="p">():</span>
    <span class="c1"># Sample N data points uniformly distributed on interval [x_min, x_max]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample_xy</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">sample_x</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_eps</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plot-two-datasets">
<h3>2.2. Plot two datasets<a class="headerlink" href="#plot-two-datasets" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We draw a random sample of two datasets of size <span class="math notranslate nohighlight">\(N\)</span></p></li>
<li><p>Plot them in different colors</p></li>
<li><p>todo: show the true function <span class="math notranslate nohighlight">\(f\)</span> along with it?  Say “in the synthetic example, we know the true <span class="math notranslate nohighlight">\(f\)</span>, which is the function we are trying to recover by learning from data”</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plt_new_fig</span><span class="p">(</span><span class="n">set_axes</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">set_axes</span><span class="p">:</span>
        <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span>    <span class="c1"># data limits</span>
        <span class="c1"># y_min, y_max = -0.5, 1.5    # data limits</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_two_datasets</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Plot two datasets on the same axes and set appropriate limits.&#39;&#39;&#39;</span>
    <span class="n">plt_new_fig</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s1">&#39;og&#39;</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s1">&#39;ok&#39;</span><span class="p">)</span>

<span class="n">x1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>
<span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>

<span class="n">plot_two_datasets</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bias_variance_decomposition_5_0.png" src="../_images/bias_variance_decomposition_5_0.png" />
</div>
</div>
</div>
<div class="section" id="fit-polynomials-of-different-degrees-to-two-datasets">
<h3>2.3. Fit polynomials of different degrees to two datasets<a class="headerlink" href="#fit-polynomials-of-different-degrees-to-two-datasets" title="Permalink to this headline">¶</a></h3>
<p>Note how the fit of the higher-order polynomial differs more (visually) from each other than the lower-order polynomial does.</p>
<p>This will visually help us understand the variance – how much the randomness in the dataset affects the learned function <span class="math notranslate nohighlight">\(\hat{f}\)</span>.  (More detailed calculation of variance to follow)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>   <span class="c1"># grid points - locations for plot and numeric integration</span>
<span class="n">degrees_to_display</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>    <span class="c1"># ingore poorly conditioned warning in polyfit</span>


<span class="k">def</span> <span class="nf">fit_and_evaluate_polynomial</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">locs</span> <span class="o">=</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degree</span><span class="p">))</span>  <span class="c1"># Fit polynomial to data using numpy package</span>
    <span class="n">ploc</span> <span class="o">=</span> <span class="n">poly</span><span class="p">(</span><span class="n">locs</span><span class="p">)</span>                           <span class="c1"># Evaluate polynomial at grid points</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ploc</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>               <span class="c1"># for numerical stability</span>


<span class="k">def</span> <span class="nf">fit_and_plot_two_datasets</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span> <span class="n">degree</span><span class="p">):</span>
    <span class="n">fhat_1</span> <span class="o">=</span> <span class="n">fit_and_evaluate_polynomial</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
    <span class="n">fhat_2</span> <span class="o">=</span> <span class="n">fit_and_evaluate_polynomial</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
    
    <span class="n">plt_new_fig</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s1">&#39;og&#39;</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">fhat_1</span><span class="p">,</span> <span class="s1">&#39;-g&#39;</span><span class="p">,</span>
             <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s1">&#39;ok&#39;</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">fhat_2</span><span class="p">,</span> <span class="s1">&#39;-k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;degree = </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">:</span>
    <span class="n">fit_and_plot_two_datasets</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bias_variance_decomposition_8_0.png" src="../_images/bias_variance_decomposition_8_0.png" />
<img alt="../_images/bias_variance_decomposition_8_1.png" src="../_images/bias_variance_decomposition_8_1.png" />
<img alt="../_images/bias_variance_decomposition_8_2.png" src="../_images/bias_variance_decomposition_8_2.png" />
<img alt="../_images/bias_variance_decomposition_8_3.png" src="../_images/bias_variance_decomposition_8_3.png" />
</div>
</div>
</div>
</div>
<div class="section" id="variance-computation">
<h2>3. Variance Computation<a class="headerlink" href="#variance-computation" title="Permalink to this headline">¶</a></h2>
<!-- ### 3.1. $E[\hat{f}]$ computation

We then approximate the expected value of the fit as the average of the fit over $M$ random dataset:
$$
E_{\mathcal{D}}[\hat{f}(x)] \ \approx \ \frac{1}{M} \sum_{i=1}^M  \hat{f}_{\mathcal{D}_i}(x)
$$
where $\hat{f}_{\mathcal{D}_i}$ is the fit to the $ith$ random dataset.

The plots below show the fits of $100$ random models and their average values,
$E_{\mathcal{D}}[\hat{f}(x)]$, for polynomials of each degree. -->
<!-- ### 3.2. Variance computation -->
<p>We now compute the variance of the fits around the mean ……  todo</p>
<!-- Note: the discrete averaging procedure is used as an approximation to the true expectation which is not easy to compute analytically. --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_fits</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_fits</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Generate random datasets and fit polynomial functions to data.&#39;&#39;&#39;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">fits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_fits</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_fits</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>
        <span class="n">fits</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">fit_and_evaluate_polynomial</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fits</span>

<span class="k">def</span> <span class="nf">compute_E_fhat</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_fits</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Approximate the average fit E[\hat{f}] by averaging over num_fits fits to random datasets.&#39;&#39;&#39;</span>
    <span class="n">fits</span> <span class="o">=</span> <span class="n">random_fits</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_fits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fits</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_variance</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_fits</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">):</span>
    <span class="n">fits</span> <span class="o">=</span> <span class="n">random_fits</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_fits</span><span class="p">)</span>
    <span class="n">variance_x</span> <span class="o">=</span> <span class="n">fits</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">variance_total</span> <span class="o">=</span> <span class="n">variance_x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">variance_total</span>

<span class="c1"># evaluate E_fhat and variance for each polynomial degree</span>
<span class="n">E_fhat</span>   <span class="o">=</span> <span class="p">{</span><span class="n">degree</span><span class="p">:</span> <span class="n">compute_E_fhat</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>   <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">}</span>    
<span class="n">variance</span> <span class="o">=</span> <span class="p">{</span><span class="n">degree</span><span class="p">:</span> <span class="n">compute_variance</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span> <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_fits_Efhat</span><span class="p">(</span><span class="n">degree</span><span class="p">):</span>
    <span class="n">fits</span> <span class="o">=</span> <span class="n">random_fits</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_fits</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>   <span class="c1"># Smaller number of fits to speed up plotting</span>
    
    <span class="n">plt_new_fig</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">fits</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">E_fhat</span><span class="p">[</span><span class="n">degree</span><span class="p">],</span>  <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;degree = </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s1">, variance = </span><span class="si">{</span><span class="n">variance</span><span class="p">[</span><span class="n">degree</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">:</span>
    <span class="n">plot_fits_Efhat</span><span class="p">(</span><span class="n">degree</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bias_variance_decomposition_11_0.png" src="../_images/bias_variance_decomposition_11_0.png" />
<img alt="../_images/bias_variance_decomposition_11_1.png" src="../_images/bias_variance_decomposition_11_1.png" />
<img alt="../_images/bias_variance_decomposition_11_2.png" src="../_images/bias_variance_decomposition_11_2.png" />
<img alt="../_images/bias_variance_decomposition_11_3.png" src="../_images/bias_variance_decomposition_11_3.png" />
</div>
</div>
</div>
<div class="section" id="bias-comptuation">
<h2>4. Bias Comptuation<a class="headerlink" href="#bias-comptuation" title="Permalink to this headline">¶</a></h2>
<p>The below plots show the expected fit <span class="math notranslate nohighlight">\(E_{\mathcal{D}}[\hat{f}]\)</span> against the true function <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>We also compute the squared bias
$<span class="math notranslate nohighlight">\(
E_x\; (f(x) - E[\hat{f}(x)])^2 \; = \;
\int_{x=0}^{2 \pi}\; p(x)\; (f(x) - E[\hat{f}(x)])^2 \; = \;
\int_{x=0}^{2 \pi}\; \frac{1}{2 \pi}\; (f(x) - E[\hat{f}(x)])^2 \; \approx \;
\sum_{i=1}^{T}\; \frac{1}{|T|}\; (f(x^{(i)}) - E[\hat{f}(x^{(i)}])^2
\)</span><span class="math notranslate nohighlight">\(
where we break up the line \)</span>[0, 2 \pi]<span class="math notranslate nohighlight">\( into \)</span>T<span class="math notranslate nohighlight">\( uniformly spaced discrete values, \)</span>x_1, \ldots, x_T$.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f_eval</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>   <span class="c1"># evaluate true function f at grid points</span>

<span class="k">def</span> <span class="nf">compute_bias_sq</span><span class="p">(</span><span class="n">degree</span><span class="p">):</span>
    <span class="n">bias_sq_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">E_fhat</span><span class="p">[</span><span class="n">degree</span><span class="p">]</span> <span class="o">-</span> <span class="n">f_eval</span><span class="p">)</span>
    <span class="n">bias_sq</span>   <span class="o">=</span> <span class="n">bias_sq_x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># * t_integral_area</span>
    <span class="k">return</span> <span class="n">bias_sq</span>

<span class="n">bias_sq</span> <span class="o">=</span> <span class="p">{</span><span class="n">degree</span><span class="p">:</span> <span class="n">compute_bias_sq</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span> <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_f_and_E_fhat</span><span class="p">(</span><span class="n">degree</span><span class="p">):</span>
    <span class="n">plt_new_fig</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">E_fhat</span><span class="p">[</span><span class="n">degree</span><span class="p">],</span> <span class="s1">&#39;-r&#39;</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">f_eval</span><span class="p">,</span> <span class="s1">&#39;-b&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;E[\hat</span><span class="si">{f}</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;degree = </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s1">, bias^2 = </span><span class="si">{</span><span class="n">bias_sq</span><span class="p">[</span><span class="n">degree</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">:</span>
    <span class="n">plot_f_and_E_fhat</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bias_variance_decomposition_14_0.png" src="../_images/bias_variance_decomposition_14_0.png" />
<img alt="../_images/bias_variance_decomposition_14_1.png" src="../_images/bias_variance_decomposition_14_1.png" />
<img alt="../_images/bias_variance_decomposition_14_2.png" src="../_images/bias_variance_decomposition_14_2.png" />
<img alt="../_images/bias_variance_decomposition_14_3.png" src="../_images/bias_variance_decomposition_14_3.png" />
</div>
</div>
</div>
<div class="section" id="total-epe">
<h2>5. Total EPE<a class="headerlink" href="#total-epe" title="Permalink to this headline">¶</a></h2>
<p>We now compute the total expeected prediction error (EPE) for each degree polynomial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Put bias, variance, irreducible error into a dataframe for easy display</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">irreducible</span> <span class="o">=</span> <span class="p">{</span><span class="n">degree</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">sigma_eps</span><span class="p">)</span> <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">}</span>
<span class="n">df_EPE</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Bias_Sq&#39;</span><span class="p">:</span> <span class="n">bias_sq</span><span class="p">,</span> <span class="s1">&#39;Variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span> <span class="s1">&#39;Irreducible&#39;</span><span class="p">:</span> <span class="n">irreducible</span><span class="p">})</span>
<span class="n">df_EPE</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;polynomial degree&#39;</span>
<span class="n">df_EPE</span><span class="p">[</span><span class="s1">&#39;EPE&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_EPE</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">df_EPE</span><span class="p">)</span>
<span class="n">df_EPE</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Bias_Sq</th>
      <th>Variance</th>
      <th>Irreducible</th>
      <th>EPE</th>
    </tr>
    <tr>
      <th>polynomial degree</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.497500</td>
      <td>0.027638</td>
      <td>0.0625</td>
      <td>0.587638</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.200318</td>
      <td>0.031793</td>
      <td>0.0625</td>
      <td>0.294611</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.004993</td>
      <td>0.023503</td>
      <td>0.0625</td>
      <td>0.090996</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.000483</td>
      <td>3.454109</td>
      <td>0.0625</td>
      <td>3.517092</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../_images/bias_variance_decomposition_16_1.png" src="../_images/bias_variance_decomposition_16_1.png" />
</div>
</div>
</div>
<div class="section" id="empirical-test-error">
<h2>6. Empirical Test Error<a class="headerlink" href="#empirical-test-error" title="Permalink to this headline">¶</a></h2>
<p>Previously we computed the theoretical EPE.
The key question is:</p>
<ul class="simple">
<li><p>Does the theoretical EPE match the emperical test error?</p></li>
</ul>
<p>To compute the emperical test error, we draw a random train and test dataset,
then perform the standard ML training and evaluation procedure:</p>
<ol class="simple">
<li><p>Draw a training set of size N</p>
<ol class="simple">
<li><p>Fit polynomials of each degree to the training data.</p></li>
</ol>
</li>
<li><p>Draw a test set of size N</p>
<ol class="simple">
<li><p>Evaluate the squared error of each fit on the test data.</p></li>
</ol>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span>   <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>

<span class="n">plot_two_datasets</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bias_variance_decomposition_18_0.png" src="../_images/bias_variance_decomposition_18_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_and_eval_test_error</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">fit_and_evaluate_polynomial</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span>
                                         <span class="n">locs</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">)</span>
    <span class="n">mean_sq_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mean_sq_err</span>

<span class="n">test_error</span> <span class="o">=</span> <span class="p">{</span><span class="n">degree</span><span class="p">:</span> <span class="n">fit_and_eval_test_error</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
                      <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">}</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">test_error</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;test error&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">S</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;polynomial degree&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>We can see that the theoretical EPE and empirical test error agree reasonbly well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_EPE</span><span class="p">[</span><span class="s1">&#39;EPE&#39;</span><span class="p">],</span> <span class="n">S</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>EPE</th>
      <th>test error</th>
    </tr>
    <tr>
      <th>polynomial degree</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.587638</td>
      <td>0.807306</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.294611</td>
      <td>0.340867</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.090996</td>
      <td>0.062224</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3.517092</td>
      <td>0.156872</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>However, the empirical error on a single dataset is obviously influenced by the random nature of the dataset.
We now compute the emperircal error by averaging over many random train and test datasets.
The emperirical error and theoretical EPE now agree <em>very</em> well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw_sample_fit_and_eval_test_error_1</span><span class="p">(</span><span class="n">degree</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    1. Draw a random training and test set</span>
<span class="sd">    2. Fit a polynomial of degree degree to the tranining data</span>
<span class="sd">    3. Evaluate performance on test set</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>
    <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span>   <span class="o">=</span> <span class="n">sample_xy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fit_and_eval_test_error</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">draw_samples_fit_and_eval_test_error</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">test_errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">draw_sample_fit_and_eval_test_error_1</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_errors</span><span class="p">)</span>

<span class="n">test_errors</span> <span class="o">=</span> <span class="p">{</span><span class="n">degree</span><span class="p">:</span> <span class="n">draw_samples_fit_and_eval_test_error</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees_to_display</span><span class="p">}</span>
<span class="n">test_errors</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">test_errors</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;test error&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">test_errors</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;polynomial degree&#39;</span>

<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_EPE</span><span class="p">[</span><span class="s1">&#39;EPE&#39;</span><span class="p">],</span> <span class="n">test_errors</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>EPE</th>
      <th>test error</th>
    </tr>
    <tr>
      <th>polynomial degree</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.587638</td>
      <td>0.589134</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.294611</td>
      <td>0.291031</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.090996</td>
      <td>0.090609</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3.517092</td>
      <td>3.426393</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="conclusion">
<h2>7. Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this notebook you learned how to:</p>
<ul class="simple">
<li><p>Comptue the theoretical expected prediction error (EPE) using the bias-variance decomposition:</p>
<ul>
<li><p>How to compute the average fit <span class="math notranslate nohighlight">\(E[\hat{f}]\)</span></p></li>
<li><p>How to compute the <strong>bias</strong> of an estimator <span class="math notranslate nohighlight">\(\hat{f}\)</span> by comparing it to the known true function <span class="math notranslate nohighlight">\(f\)</span></p></li>
<li><p>How to compute the <strong>variance</strong> of an estimator</p></li>
</ul>
</li>
<li><p>Compare bias-variance over models of different complexity</p>
<ul>
<li><p>Using the total EPE plots.  Note how they agree well with the “ideal” bias-variance curves shown at the beginning of the chapter.</p></li>
</ul>
</li>
<li><p>Theoretical EPE and emperical error agree</p>
<ul>
<li><p>Compare the error predicted by EPE with emperical error using standard ML procedure (train and evaluate) on test data.</p></li>
</ul>
</li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../chapter4/regularization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Regularization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="validation_methods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Cross-validation Methods</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Andrew Wolf<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>