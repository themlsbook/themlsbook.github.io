{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4a8f3f",
   "metadata": {},
   "source": [
    "(chapter4_part2)=\n",
    "\n",
    "# Regularization\n",
    "\n",
    "- This Jupyter Notebook is a supplement for the [Machine Learning Simplified](https://themlsbook.com) (MLS) book. Note that all detailed explanations are written in the book. This notebook just shed light on Python implementations of the topics discussed.\n",
    "- I also assume you know Python syntax and how it works. If you don't, I highly recommend you to take a break and get introduced to the language before going forward with my notebooks. \n",
    "\n",
    "## 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68830550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to automatically create polynomial features! \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Import Linear Regression and a regularized regression function\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "# Finally, import function to make a machine learning pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08470b2",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "\n",
    "#Confusingly, the lambda term can be configured via the “alpha” argument when defining the class. The default value is 1.0 or a full penalty.\n",
    "\n",
    "degree_=4\n",
    "lambda_=0.5\n",
    "\n",
    "# scale the X data to prevent numerical errors.\n",
    "X_train = np.array(X_train).reshape(-1, 1)\n",
    "\n",
    "polyX = PolynomialFeatures(degree=degree_).fit_transform(X_train)\n",
    "\n",
    "model1 = LinearRegression().fit(polyX, y_train)\n",
    "model2 = Ridge(alpha=lambda_).fit(polyX, y_train)\n",
    "\n",
    "# print(\"OLS Coefs: \" + str(model1.coef_[0]))\n",
    "# print(\"Ridge Coefs: \" + str(model2.coef_[0]))\n",
    "\n",
    "print(f\"Linear Coefs: {sum(model1.coef_)}\")\n",
    "print(f\"Ridge Coefs: {sum(model2.coef_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = np.array(np.linspace(0, 120, 120)).reshape(-1, 1)\n",
    "t = PolynomialFeatures(degree=degree_).fit_transform(t_)\n",
    "\n",
    "\n",
    "\n",
    "# visualize\n",
    "plt.plot(X_train, y_train, 'o', t, model2.predict(t), '-')\n",
    "# plt.scatter(X_train, y_train, color='blue', label='Training set')\n",
    "# plt.scatter(X_test, y_test, color='red', label='Test set')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim((0,120))\n",
    "plt.xlim((0,120))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddade37",
   "metadata": {},
   "source": [
    "## 3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991aeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "\n",
    "#Confusingly, the lambda term can be configured via the “alpha” argument when defining the class. The default value is 1.0 or a full penalty.\n",
    "\n",
    "degree_=4\n",
    "lambda_=0\n",
    "\n",
    "\n",
    "# scale the X data to prevent numerical errors.\n",
    "X_train = np.array(X_train).reshape(-1, 1)\n",
    "# y_train = np.array(y_train).reshape(-1, 1)\n",
    "\n",
    "polyX = PolynomialFeatures(degree=degree_).fit_transform(X_train)\n",
    "\n",
    "model1 = LinearRegression().fit(polyX, y_train)\n",
    "model2 = Lasso(alpha=lambda_, max_iter=1300000).fit(polyX, y_train)\n",
    "\n",
    "print(f\"Linear Coefs: {model1.coef_}\")\n",
    "print(f\"Lasso Coefs: {model2.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(model2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a752b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = np.array(np.linspace(0, 120, 120)).reshape(-1, 1)\n",
    "t = PolynomialFeatures(degree=degree_).fit_transform(t_)\n",
    "\n",
    "\n",
    "\n",
    "# visualize\n",
    "plt.plot(X_train, y_train, 'o', t, model2.predict(t), '-')\n",
    "# plt.scatter(X_train, y_train, color='blue', label='Training set')\n",
    "# plt.scatter(X_test, y_test, color='red', label='Test set')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim((0,120))\n",
    "plt.xlim((0,120))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac02d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11,
   23,
   35,
   40,
   67,
   81,
   86,
   112,
   117,
   134
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}